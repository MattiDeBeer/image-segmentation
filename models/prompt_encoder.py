#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr  4 17:34:49 2025

@author: jamie
"""
import torch
import torch.nn as nn
from models.processing_blocks import ConvBlockDownsample, ConvBlockUpsampleSkip, ConvBlock, ConvBlockUpsample
from models.autoencoder import Autoencoder
from models.pre_trained import SegmentationDecoderSkip
class PromptEncoder(nn.Module):
    def __init__(self, out_channels=512):
        super().__init__()
        self.enc1 = ConvBlockDownsample(1, 32)    # Input is single-channel heatmap
        self.enc2 = ConvBlockDownsample(32, 64)
        self.enc3 = ConvBlockDownsample(64, out_channels)  # Match image bottleneck

    def forward(self, x):
        x1 = self.enc1(x)
        x2 = self.enc2(x1)
        x3 = self.enc3(x2)
        return x3  # Shape: [B, out_channels, H/8, W/8]
    
    
    
class SegmentationModelWithPrompt(nn.Module):
    def __init__(self, image_encoder, prompt_encoder, decoder, fusion_method='concat'):
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.decoder = decoder
        self.fusion_method = fusion_method

        if fusion_method == 'concat':
            # Assuming image_encoder produces 512 channels and prompt_encoder also outputs 512,
            # concatenation gives 1024 channels. We project back to 512 channels.
            self.fuse_conv = nn.Conv2d(1024, 512, kernel_size=1)
        elif fusion_method == 'add':
            # For addition, both must have the same number of channels.
            pass

    def forward(self, image, prompt):
        features = self.image_encoder(image)  # e.g., returns {"bottleneck": [B, 512, H, W], ...}
        prompt_feat = self.prompt_encoder(prompt)  # Expected shape: [B, 512, H, W] (or upsampled to match)
        if self.fusion_method == 'concat':
            fused = torch.cat([features["bottleneck"], prompt_feat], dim=1)  # [B, 1024, H, W]
            fused = self.fuse_conv(fused)  # Now [B, 512, H, W]
        elif self.fusion_method == 'add':
            fused = features["bottleneck"] + prompt_feat
        else:
            raise ValueError("Invalid fusion method specified")
        features["bottleneck"] = fused
        seg_out = self.decoder(features)
        return seg_out


if __name__ == "__main__":
    # Load the autoencoder and freeze its encoder.
    autoencoder = Autoencoder(in_channels=3, out_channels=3)
    autoencoder.load_state_dict(torch.load("../saved_models/Autoencoder/run_008/model5.pth"))
    for param in autoencoder.encoder.parameters():
        param.requires_grad = False

    # Instantiate the prompt encoder (trainable).
    prompt_encoder = PromptEncoder(out_channels=512)  # Designed to output [B,512,H/8,W/8]
    
    # Use the existing segmentation decoder (skip version).
    decoder = SegmentationDecoderSkip(out_channels=3)
    
    # Create the segmentation model with prompt fusion.
    seg_model = SegmentationModelWithPrompt(
        image_encoder=autoencoder.encoder,
        prompt_encoder=prompt_encoder,
        decoder=decoder,
        fusion_method='concat'
    )
    
    # Test the model with dummy inputs:
    image = torch.randn(1, 3, 256, 256)   # Example image
    prompt = torch.randn(1, 1, 256, 256)   # Example prompt heatmap (should be generated by your dataset normally)
    out = seg_model(image, prompt)
    print("Segmentation output shape:", out.shape)